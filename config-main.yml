data_kwargs:
  dataset: translation

  train_ds_kwargs:
    src_path: ../data/iwslt_en_vi/train.en
    trg_path: ../data/iwslt_en_vi/train.vi 
    src_max_len: 64
    trg_max_len: 64

  valid_ds_kwargs:
    src_path: ../data/iwslt_en_vi/tst2013.en
    trg_path: ../data/iwslt_en_vi/tst2013.vi

  train_batch_sz: 64
  valid_batch_sz: 64

  src_vocab_min_freq: 2
  trg_vocab_min_freq: 2
  # src_max_size: 8000
  # trg_max_size: 8000


model: transformer_nmt
model_kwargs:
  # Architecture config
  d_model: 512
  nhead: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  dim_ff: 2048
  dropout: 0.1
  activation: relu
  max_input_length: 200
  

# Config for training
train_config:
  optimizer: adam
  optimizer_kwargs:
    beta1: 0.9
    beta2: 0.98
    lr: 0.2
    eps: !!float 1e-9

  scheduler: noam
  scheduler_kwargs:
    scalar: 0.2
    warmup_steps: 4000
    d_model: 512
  
  loss_metric: label_smoothing_loss
  loss_kwargs:
    label_smoothing: 0.1

  n_epochs: 20
  report_steps: 200
  valid_epochs: 1
  eval_epochs: 1
  save_checkpoint_epochs: 2
  use_float32: true

  eval_metric: bleu
  strategy: beam_search
  strategy_kwargs:
    k: 4
    max_len: 160
    alpha: 0.6
    beta: 0.0

infer_config:
  strategy: beam_search
  batch_size: 64
  strategy_kwargs:
    k: 4
    max_len: 160
    alpha: 0.6
    beta: 0.0

device: cuda
keep_checkpoints: 5

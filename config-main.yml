build_vocab_config:
  src_paths:
    - /home/datnt/data/en_vi_IWSLT/train.clean.en
  trg_paths:
    - /home/datnt/data/en_vi_IWSLT/train.clean.vi
  src_min_freq: 2
  trg_min_freq: 2
  # src_max_size: 32000
  # trg_max_size: 32000
  src_specials: 
    - <pad>
    - <unk>
  trg_specials:
    - <pad>
    - <unk>
    - <sos>
    - <eos>

model: transformer_nmt
model_config:
  # Architecture config
  share_embed: false
  d_model: 512
  nhead: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  dim_ff: 2048
  dropout: 0.1
  activation: relu
  max_input_length: 200

# Config for training
train_config:
  lazy: false
  train_loader:
    batch_size: 64
    # n_tokens: 4000
    en_vi_IWSLT:
      src_lang: en
      trg_lang: vi
      # src_prefix: <en> # (default: "")
      # trg_prefix: <vi> # (default: <sos>)
      src_max_len: 64
      trg_max_len: 64
      path: /home/datnt/data/en_vi_IWSLT/train.clean

  valid_loader:
    batch_size: 64
    # n_tokens: 4000
    en_vi_IWSLT:
      src_lang: en
      trg_lang: vi
      path: /home/datnt/data/en_vi_IWSLT/tst2013.clean

  accum_steps: 1

  optimizer: adam
  optimizer_kwargs:
    beta1: 0.9
    beta2: 0.98
    lr: 0.2
    eps: !!float 1e-9

  scheduler: noam
  scheduler_kwargs:
    scalar: 1.0
    warmup_steps: 4000
    d_model: 512
  
  loss_metric: label_smoothing_loss
  loss_kwargs:
    label_smoothing: 0.1

  n_epochs: 50
  # n_steps: 50000
  report_steps: 100
  valid_epochs: 1
  valid_steps: 1000
  eval_epochs: 1
  eval_steps: 1000
  save_checkpoint_epochs: 1
  save_checkpoint_steps: 2000
  use_float32: false

  eval_metric: bleu

strategy: beam_search
strategy_config:
  k: 4
  max_len: 160
  alpha: 0.6
  beta: 0.0
  support_kwargs:
    use_actives: false
    use_cache: true

device: cuda
keep_checkpoints: 10

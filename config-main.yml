data_kwargs:
  dataset: translation

  train_ds_kwargs:
    src_path: ../data/iwslt_en_vi/train.en
    trg_path: ../data/iwslt_en_vi/train.vi 
    src_max_len: 100
    trg_max_len: 100  

  valid_ds_kwargs:
    src_path: ../data/iwslt_en_vi/tst2013.en
    trg_path: ../data/iwslt_en_vi/tst2013.vi

  train_batch_sz: 32
  valid_batch_sz: 32

  src_vocab_min_freq: 2
  trg_vocab_min_freq: 2
  # src_max_size: 8000
  # trg_max_size: 8000


model: transformer_nmt
model_kwargs:
  # Architecture config
  d_model: 512
  nhead: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  dim_ff: 2048
  dropout: 0.1
  activation: relu
  max_input_length: 300
  

# Config for training
train_config:
  optimizer: adam
  optimizer_kwargs:
    beta1: 0.9
    beta2: 0.98
    lr: 0.2
    eps: !!float 1e-9

  scheduler: noam
  scheduler_kwargs:
    scalar: 0.2
    warmup_steps: 4000
    d_model: 512
  
  loss_metric: label_smoothing_loss
  loss_kwargs:
    label_smoothing: 0.1

  n_epochs: 50
  report_steps: 200
  valid_epochs: 1
  eval_epochs: 1
  save_checkpoint_epochs: 1

  eval_metric: bleu
  strategy: beam_search
  strategy_kwargs:
    k: 4
    max_len: 160

infer_config:
  strategy: beam_search
  batch_size: 32
  strategy_kwargs:
    k: 4
    max_len: 160

device: cuda
